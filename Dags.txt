from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email_operator import EmailOperator
from datetime import timedelta, datetime
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

default_args={
        'owner':'Lahasya',
        'start_date':datetime(2023,7,7),
        'retries':3,
        'retry_delay':timedelta(minutes=5)
        }

dag= DAG('timberland_analysis',
          default_args=default_args,
          description='airflow project',
          schedule_interval='* * * * *',
          catchup=False,
          tags=['example,analysis']
          )

start_task = DummyOperator(task_id='start_task',dag=dag)



# Define the Spark job
spark_job = SparkSubmitOperator(
    task_id='spark_job',
    application='/root/airflow/dags/dags_spark.py',
    connection_id='spark_default',
    verbose=1,
    dag=dag
)


end_task = DummyOperator(task_id='end_task',dag=dag)

email_task = EmailOperator(
        task_id='email_task',
        to=['lahasya24@gmail.com'],
        subject="Spark job has run successfully",
        html_content="<b>output generated by spark in airflow</b>",
        dag=dag
)


start_task >> spark_job >> end_task >> email_task